{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "# [MNIST Dataset Source](http://yann.lecun.com/exdb/mnist/)\n",
    "# [I am following this YouTube tutorial](https://www.youtube.com/watch?v=2FmcHiLCwTU&vl=en)\n",
    "\n",
    "### Goal \n",
    "* Build a classifier that can look at a 28x28 image of a handwritten digit and classify the digit (0-9).\n",
    "  * the \"Hello World\" of Deep Learning\n",
    "* Personal goals: \n",
    "  * understand Tensorflow's python wrapper & Tensorflow a little bit better\n",
    "  * understand neural networks a little bit better\n",
    "  * understand some basics of machine learning a little bit better\n",
    "\n",
    "### Tools\n",
    "* Tensorflow\n",
    "* Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "6fbc9c6fd2b6328f807d4a5b19dc22b026b02317",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import input_data # standard python class for downloading datasets\n",
    "# read MNIST data\n",
    "# https://stackoverflow.com/a/37540230/5411712\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_Data\", one_hot=True)\n",
    "print(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af27b0aa16f5892320f93438c46bde72f4dfcf6f"
   },
   "source": [
    "# This is what the data looks like:\n",
    "![mnist_data](https://i.imgur.com/mKstG9R.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "00992bb41e285831d8491e447367fbfad1507a96"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1451d4aa15a1b7cd5e1947887fe75d78014ec9da"
   },
   "source": [
    "# set \"hyperparameters\" (knobs & dials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e89023e6bfa996f481e956659619271388038332"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01 # how fast to update weights; 0.01 is standard and pretty good\n",
    "        # too big >> miss optimal soln; too small >> takes too long to find optimal soln\n",
    "training_iteration = 30 # number of times to run the gradient descent (optimizer) step\n",
    "batch_size = 100\n",
    "display_step = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "12da831b81c1799bcc61e09ef1d6f7d2450bbc37"
   },
   "source": [
    "## Learning Rate:\n",
    "![learning_rate](https://i.imgur.com/3L1qbdT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b2ae79f7c5a4b333f465f68f7c41b0ed3e12236"
   },
   "source": [
    "## notes\n",
    "#### tensorflow\n",
    "* Tensorflow \"model\" = \"data flow graph\"\n",
    "* Graph has nodes called \"operations\"\n",
    "  * basic units of math (e.g: addition, multiplication, fancy-schmancy-multivar-calculus, etc)\n",
    "  * input: tensor\n",
    "  * output: tensor\n",
    "* tensor = multidimensional arrays (matrices)\n",
    "\n",
    "#### conventions\n",
    "* x = feature vector / the thing(s) that help us do the prediction\n",
    "* y = \"output classes\" / the thing we want to predict\n",
    "* \"**placeholder**\" = a variable that will have data assigned to it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9652842aa63b13a80edfb30a4712872d05d4f77d"
   },
   "outputs": [],
   "source": [
    "# TF graph input\n",
    "x = tf.placeholder(\"float\", [None, 784]) # mnist data image of shape; 28*28=784\n",
    "     # notice images are 28px by 28px arrays & get \"flattened\" into 1D array of 784 pixels\n",
    "y = tf.placeholder(\"float\", [None, 10]) # 0-9 digits recognition >> 10 classes to be \"classified\"\n",
    "\n",
    "# create a model\n",
    "\n",
    "# set model parameters\n",
    "W = tf.Variable(tf.zeros([784, 10])) # weights (probabilities that affect how data flows in graph)\n",
    "b = tf.Variable(tf.zeros([10]))      # biases (lets us shift the regression line to fit data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3dbdd64c4bd206105b0bde2dfbbb4c0b5309fd40"
   },
   "source": [
    "# An image is represented as a matrix of pixel values:\n",
    "![Imgur](https://i.imgur.com/XYyI1ha.png)\n",
    "\n",
    "# It gets flattened into a 1D array to be used as the feature vector:\n",
    "![Imgur](https://i.imgur.com/d9ZvYPV.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4af534b2dcb75c760f19cf1b9c6b10b0c1d4973d"
   },
   "outputs": [],
   "source": [
    "# \"scopes help us organize nodes in the graph visualizer called, Tensorboard\"\n",
    "with tf.name_scope(\"Wx_b\") as scope:\n",
    "    # First scope constructs a linear model (Logistic Regression)\n",
    "    # `tf.nn` --- https://www.tensorflow.org/api_docs/python/tf/nn\n",
    "    model = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax???? what about ReLU? Sigmoid? \n",
    "                                               # tf.nn.relu(biases=,features=,name=,weights=,x=)\n",
    "                                               # tf.nn.softmax(_sentinel=,axis=,dim=,labels=,logits=,name=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9f6e8a63a6ab47f5becfd0d74262a0e302e589e"
   },
   "source": [
    "# Logistic Regression:\n",
    "![Imgur](https://i.imgur.com/rrkOONc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de95d9c03320163d06bf889210f29da65231e346"
   },
   "outputs": [],
   "source": [
    "# Add summary operations to collect data\n",
    "# helps us later visualize the distribution of the Weights and biases\n",
    "# https://github.com/tensorflow/serving/issues/270\n",
    "w_h = tf.summary.histogram(\"weights\", W)\n",
    "b_h = tf.summary.histogram(\"biases\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0d0f5aa525e5e84cbf1f428d240dbdfa94c51627"
   },
   "outputs": [],
   "source": [
    "# More name scopes will clean up graph representation\n",
    "with tf.name_scope(\"cost_function\") as scope:\n",
    "    # Second scope minimizes error using \"cross entropy function\" as the \"cost function\"\n",
    "    # cross entropy function\n",
    "    cost_function = -tf.reduce_sum(y*tf.log(model))\n",
    "    # create a summary to monitor the cost function; for later visualization\n",
    "    tf.summary.scalar(\"cost_function\", cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0c53b9f3d9839ed176245e023bad4a6319c7a9ca"
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\") as scope:\n",
    "    # Last scope Gradient Descent; the training algorithm\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad0dcf8a9e168f93d043502a579b2929fd2edea5"
   },
   "source": [
    "# Gradient Descent:\n",
    "![Imgur](https://i.imgur.com/i6WW4gH.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "158cd19c057cb7ce9a2f4b9e6a894bd2e92b006e"
   },
   "outputs": [],
   "source": [
    "# initialize the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc3072140a3deb7238f50763352d2b9e9a9dc75c"
   },
   "outputs": [],
   "source": [
    "# merge summaries into 1 operation\n",
    "# https://github.com/tensorflow/tensorflow/issues/7737\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5047bbf2151c312c6ab32f564044dc7bcfac5120"
   },
   "outputs": [],
   "source": [
    "print(\"learning_rate\\t\\t=\\t\" + str(learning_rate))\n",
    "print(\"training_iteration\\t=\\t\" + str(training_iteration))\n",
    "print(\"batch_size\\t\\t=\\t\" + str(batch_size))\n",
    "print(\"display_step\\t\\t=\\t\" + str(display_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7501dc27cb1d39e80ad7fdc89ad8b023fce6b933"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dfb3f193589c6be25e1557c1882686019414825f"
   },
   "outputs": [],
   "source": [
    "# Start training by launching a session that executes the data flow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Set the logs writer to the folder /tmp/tensorflow_logs\n",
    "    # This is for all the visualizations later\n",
    "    # https://stackoverflow.com/a/41483033/5411712\n",
    "    summary_writer = tf.summary.FileWriter('./logs', graph_def=sess.graph_def)\n",
    "    \n",
    "    # Training cycle\n",
    "    for i in range(training_iteration):\n",
    "        avg_cost = 0.0 # prints out periodically to make sure model is \"improving\" ... goal is to minimize cost\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        # loop over all batches\n",
    "        for b in range(total_batch): # for each example in training data\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # fit training using batch data\n",
    "            # `optimizer` is Gradient Descent; used for 'backpropagation'\n",
    "            sess.run(optimizer, feed_dict={x:batch_xs, y:batch_ys})\n",
    "            # compute the average loss\n",
    "            avg_cost += sess.run(cost_function, feed_dict={x:batch_xs, y:batch_ys})/total_batch\n",
    "            # write logs for each iteration\n",
    "            summary_str = sess.run(merged_summary_op, feed_dict={x:batch_xs, y:batch_ys})\n",
    "            summary_writer.add_summary(summary_str, i * total_batch + b)\n",
    "                                            # why `i * total_batch + b` ??? idk.\n",
    "        # Display logs per iteration step\n",
    "        if (i % display_step == 0):\n",
    "            print(\"iteration:\", '%04d' % (i+1), \"avg_cost=\", \"{:9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"\\nTraining completed!\\n\")\n",
    "\n",
    "    # Test the model\n",
    "    # remember 'y' is the prediction variable\n",
    "    predictions = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e0fb76720eea3111eb86c2535f5bfb123a16102"
   },
   "source": [
    "## Notice that the ```avg_cost``` values decrease with each logged iteration. This means that the gradient descent algorithm is minimizing the cost function. \n",
    "### I suppose if we ran the code with ```training_iteration``` set to a larger number then we would expect to see little to no improvement on the accuracy since the ```avg_cost``` seems to level off at around 18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28ccf2e6d9380ba2ce4fe30e8aa3e59a6e906033"
   },
   "source": [
    "# Viewing all the summaries in ***Tensorboard***\n",
    "##### this should be done locally so make sure to **download** the ```kernal.ipynb``` file and run ```tensorboard --logdir=./logs``` in the command line \n",
    "* ***Note:*** ```pip install tensorflow``` may be required to import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "98d215136d685bbf1e93f85e3b39b213e105bc9c"
   },
   "source": [
    "#### main graph\n",
    "<img src=\"https://i.imgur.com/f8LgApJ.png\" width=\"400\">\n",
    "#### tensorboard_auxilary_nodes\n",
    "![tensorboard_auxilary_nodes](https://i.imgur.com/ZABjzeR.png)\n",
    "#### tensorboard_cost_function\n",
    "![tensorboard_cost_function](https://i.imgur.com/yTCklib.png)\n",
    "#### tensorboard_biases_distribution\n",
    "![tensorboard_biases_distribution](https://i.imgur.com/iyZupnI.png)\n",
    "#### tensorboard_weights_distribution\n",
    "![tensorboard_weights_distribution](https://i.imgur.com/DxgMGZt.png)\n",
    "#### tensorboard_biases_histogram\n",
    "![tensorboard_biases_histogram](https://i.imgur.com/Af06kgc.png)\n",
    "#### tensorboard_weights_histogram\n",
    "![tensorboard_weights_histogram](https://i.imgur.com/wcbcIdy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "959dd1f641e9dad526cd8e5299603db20d2a59ec"
   },
   "outputs": [],
   "source": [
    "# optionally run the command in the notebook itself by uncommenting the line below\n",
    "#!tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e181622ff4f8222f80cb836d700b2b0c941abcbe"
   },
   "source": [
    "# Future Learning\n",
    "* What is PyTorch and how does it compare to Tensorflow?\n",
    "   * https://www.youtube.com/watch?v=nbJ-2G2GXL0\n",
    "     * Would PyTorch reduce the need to define \"placeholders\" because that was, frankly, weird to see in a language like python?\n",
    "* [But what *is* a Neural Network?](https://youtu.be/aircAruvnKk)\n",
    "  * [and how do they learn?](https://youtu.be/IHZwWFHWa-w) \n",
    "  * [and what is backprop really doing?](https://youtu.be/Ilg3gGewQ5U)\n",
    "  * [and how does backprop use calculus?](https://youtu.be/tIeHLnjs5U8)\n",
    "* How can the accuracy found above ```0.9254``` be improved to closer to ```0.95``` or ```0.99```?\n",
    "   * To what extend does changing the ```learning_rate``` or ```training_iteration``` or ```batch_size``` affect the accuracy? \n",
    "     * I dont think batch_size should have any affect. \n",
    "     * with ```training_iteration=30``` and ```learning_rate=0.01``` the algorithm ran in less than a few minutes and achieved ```0.9254```. Perhaps allowing it to train for several hours would boost the accuracy?\n",
    "       * [relevant quora question](http://qr.ae/TUGJid)\n",
    "* How long would it take a human toddler to \"classify\" digits (0-9)? An hour or two? maybe less? Of course you would need to hold their attention to the task, haha! \n",
    "* What would happen to the accuracy if I modified the test data or the training data to include **random noise** or even attempt the [**one pixel attack**](https://arxiv.org/abs/1710.08864)\n",
    "  * [video about one pixel attack](https://youtu.be/SA4YEAWVpbk)\n",
    ">   \"Now, note that this also means that we have to be able to look into the neural network and have access to the confidence values.\" - [KÃ¡roly Zsolnai-FehÃ©r](https://youtu.be/SA4YEAWVpbk?t=155)\n",
    "  *  Would a method for reducing NerualNet accuracy that only sees output classes, *without accuracy values*, be analogous to humans discovering optical illusions? haha! ðŸ˜‚\n",
    "  * [it seems some researchers have tried to trick AI that learned on MNIST data](https://arxiv.org/pdf/1801.02612.pdf)\n",
    "  * [another one](https://arxiv.org/pdf/1608.04644.pdf)\n",
    "  * [and another one](https://arxiv.org/pdf/1807.10335.pdf)\n",
    "  * [and more](https://www.google.com/search?safe=off&q=one+pixel+attack+\"mnist\")\n",
    "* What would happen to the accuracy if I changed **```tf.nn.softmax```** to **```relu```** or even **```sigmoid```** or **```tanh```**?\n",
    "  * [learn more about activation functions](https://youtu.be/-7scQpJT7uo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
