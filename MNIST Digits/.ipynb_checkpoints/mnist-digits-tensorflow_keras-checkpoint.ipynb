{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# [MNIST Dataset Source](http://yann.lecun.com/exdb/mnist/)\n# [I am following this YouTube tutorial](https://www.youtube.com/watch?v=2FmcHiLCwTU&vl=en)\n\n### Goal \n* Build a classifier that can look at a 28x28 image of a handwritten digit and classify the digit (0-9).\n  * the \"Hello World\" of Deep Learning\n* Personal goals: \n  * understand Tensorflow's python wrapper & Tensorflow a little bit better\n  * understand neural networks a little bit better\n  * understand some basics of machine learning a little bit better\n\n### Tools\n* Tensorflow\n* Python"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"6fbc9c6fd2b6328f807d4a5b19dc22b026b02317","_kg_hide-output":true},"cell_type":"code","source":"# import input_data # standard python class for downloading datasets\n# read MNIST data\n# https://stackoverflow.com/a/37540230/5411712\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_Data\", one_hot=True)\nprint(mnist)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af27b0aa16f5892320f93438c46bde72f4dfcf6f"},"cell_type":"markdown","source":"# This is what the data looks like:\n![mnist_data](https://i.imgur.com/mKstG9R.png)"},{"metadata":{"trusted":true,"_uuid":"00992bb41e285831d8491e447367fbfad1507a96"},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1451d4aa15a1b7cd5e1947887fe75d78014ec9da"},"cell_type":"markdown","source":"# set \"hyperparameters\" (knobs & dials)"},{"metadata":{"trusted":true,"_uuid":"e89023e6bfa996f481e956659619271388038332"},"cell_type":"code","source":"learning_rate = 0.01 # how fast to update weights; 0.01 is standard and pretty good\n        # too big >> miss optimal soln; too small >> takes too long to find optimal soln\ntraining_iteration = 30 # number of times to run the gradient descent (optimizer) step\nbatch_size = 100\ndisplay_step = 2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12da831b81c1799bcc61e09ef1d6f7d2450bbc37"},"cell_type":"markdown","source":"## Learning Rate:\n![learning_rate](https://i.imgur.com/3L1qbdT.png)"},{"metadata":{"_uuid":"5b2ae79f7c5a4b333f465f68f7c41b0ed3e12236"},"cell_type":"markdown","source":"## notes\n#### tensorflow\n* Tensorflow \"model\" = \"data flow graph\"\n* Graph has nodes called \"operations\"\n  * basic units of math (e.g: addition, multiplication, fancy-schmancy-multivar-calculus, etc)\n  * input: tensor\n  * output: tensor\n* tensor = multidimensional arrays (matrices)\n\n#### conventions\n* x = feature vector / the thing(s) that help us do the prediction\n* y = \"output classes\" / the thing we want to predict\n* \"**placeholder**\" = a variable that will have data assigned to it later"},{"metadata":{"trusted":true,"_uuid":"9652842aa63b13a80edfb30a4712872d05d4f77d"},"cell_type":"code","source":"# TF graph input\nx = tf.placeholder(\"float\", [None, 784]) # mnist data image of shape; 28*28=784\n     # notice images are 28px by 28px arrays & get \"flattened\" into 1D array of 784 pixels\ny = tf.placeholder(\"float\", [None, 10]) # 0-9 digits recognition >> 10 classes to be \"classified\"\n\n# create a model\n\n# set model parameters\nW = tf.Variable(tf.zeros([784, 10])) # weights (probabilities that affect how data flows in graph)\nb = tf.Variable(tf.zeros([10]))      # biases (lets us shift the regression line to fit data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3dbdd64c4bd206105b0bde2dfbbb4c0b5309fd40"},"cell_type":"markdown","source":"# An image is represented as a matrix of pixel values:\n![Imgur](https://i.imgur.com/XYyI1ha.png)\n\n# It gets flattened into a 1D array to be used as the feature vector:\n![Imgur](https://i.imgur.com/d9ZvYPV.png)"},{"metadata":{"trusted":true,"_uuid":"4af534b2dcb75c760f19cf1b9c6b10b0c1d4973d"},"cell_type":"code","source":"# \"scopes help us organize nodes in the graph visualizer called, Tensorboard\"\nwith tf.name_scope(\"Wx_b\") as scope:\n    # First scope constructs a linear model (Logistic Regression)\n    # `tf.nn` --- https://www.tensorflow.org/api_docs/python/tf/nn\n    model = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax???? what about ReLU? Sigmoid? \n                                               # tf.nn.relu(biases=,features=,name=,weights=,x=)\n                                               # tf.nn.softmax(_sentinel=,axis=,dim=,labels=,logits=,name=)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9f6e8a63a6ab47f5becfd0d74262a0e302e589e"},"cell_type":"markdown","source":"# Logistic Regression:\n![Imgur](https://i.imgur.com/rrkOONc.png)"},{"metadata":{"trusted":true,"_uuid":"de95d9c03320163d06bf889210f29da65231e346"},"cell_type":"code","source":"# Add summary operations to collect data\n# helps us later visualize the distribution of the Weights and biases\n# https://github.com/tensorflow/serving/issues/270\nw_h = tf.summary.histogram(\"weights\", W)\nb_h = tf.summary.histogram(\"biases\", b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d0f5aa525e5e84cbf1f428d240dbdfa94c51627"},"cell_type":"code","source":"# More name scopes will clean up graph representation\nwith tf.name_scope(\"cost_function\") as scope:\n    # Second scope minimizes error using \"cross entropy function\" as the \"cost function\"\n    # cross entropy function\n    cost_function = -tf.reduce_sum(y*tf.log(model))\n    # create a summary to monitor the cost function; for later visualization\n    tf.summary.scalar(\"cost_function\", cost_function)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c53b9f3d9839ed176245e023bad4a6319c7a9ca"},"cell_type":"code","source":"with tf.name_scope(\"train\") as scope:\n    # Last scope Gradient Descent; the training algorithm\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad0dcf8a9e168f93d043502a579b2929fd2edea5"},"cell_type":"markdown","source":"# Gradient Descent:\n![Imgur](https://i.imgur.com/i6WW4gH.png)"},{"metadata":{"trusted":true,"_uuid":"158cd19c057cb7ce9a2f4b9e6a894bd2e92b006e"},"cell_type":"code","source":"# initialize the variables\ninit = tf.initialize_all_variables()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc3072140a3deb7238f50763352d2b9e9a9dc75c"},"cell_type":"code","source":"# merge summaries into 1 operation\n# https://github.com/tensorflow/tensorflow/issues/7737\nmerged_summary_op = tf.summary.merge_all()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5047bbf2151c312c6ab32f564044dc7bcfac5120"},"cell_type":"code","source":"print(\"learning_rate\\t\\t=\\t\" + str(learning_rate))\nprint(\"training_iteration\\t=\\t\" + str(training_iteration))\nprint(\"batch_size\\t\\t=\\t\" + str(batch_size))\nprint(\"display_step\\t\\t=\\t\" + str(display_step))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7501dc27cb1d39e80ad7fdc89ad8b023fce6b933"},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true,"_uuid":"dfb3f193589c6be25e1557c1882686019414825f"},"cell_type":"code","source":"# Start training by launching a session that executes the data flow graph\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # Set the logs writer to the folder /tmp/tensorflow_logs\n    # This is for all the visualizations later\n    # https://stackoverflow.com/a/41483033/5411712\n    summary_writer = tf.summary.FileWriter('./logs', graph_def=sess.graph_def)\n    \n    # Training cycle\n    for i in range(training_iteration):\n        avg_cost = 0.0 # prints out periodically to make sure model is \"improving\" ... goal is to minimize cost\n        total_batch = int(mnist.train.num_examples / batch_size)\n        # loop over all batches\n        for b in range(total_batch): # for each example in training data\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n            # fit training using batch data\n            # `optimizer` is Gradient Descent; used for 'backpropagation'\n            sess.run(optimizer, feed_dict={x:batch_xs, y:batch_ys})\n            # compute the average loss\n            avg_cost += sess.run(cost_function, feed_dict={x:batch_xs, y:batch_ys})/total_batch\n            # write logs for each iteration\n            summary_str = sess.run(merged_summary_op, feed_dict={x:batch_xs, y:batch_ys})\n            summary_writer.add_summary(summary_str, i * total_batch + b)\n                                            # why `i * total_batch + b` ??? idk.\n        # Display logs per iteration step\n        if (i % display_step == 0):\n            print(\"iteration:\", '%04d' % (i+1), \"avg_cost=\", \"{:9f}\".format(avg_cost))\n\n    print(\"\\nTraining completed!\\n\")\n\n    # Test the model\n    # remember 'y' is the prediction variable\n    predictions = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n    # Calculate accuracy\n    accuracy = tf.reduce_mean(tf.cast(predictions, \"float\"))\n    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e0fb76720eea3111eb86c2535f5bfb123a16102"},"cell_type":"markdown","source":"## Notice that the ```avg_cost``` values decrease with each logged iteration. This means that the gradient descent algorithm is minimizing the cost function. \n### I suppose if we ran the code with ```training_iteration``` set to a larger number then we would expect to see little to no improvement on the accuracy since the ```avg_cost``` seems to level off at around 18."},{"metadata":{"_uuid":"28ccf2e6d9380ba2ce4fe30e8aa3e59a6e906033"},"cell_type":"markdown","source":"# Viewing all the summaries in ***Tensorboard***\n##### this should be done locally so make sure to **download** the ```kernal.ipynb``` file and run ```tensorboard --logdir=./logs``` in the command line \n* ***Note:*** ```pip install tensorflow``` may be required to import tensorflow"},{"metadata":{"_uuid":"98d215136d685bbf1e93f85e3b39b213e105bc9c"},"cell_type":"markdown","source":"#### main graph\n<img src=\"https://i.imgur.com/f8LgApJ.png\" width=\"400\">\n#### tensorboard_auxilary_nodes\n![tensorboard_auxilary_nodes](https://i.imgur.com/ZABjzeR.png)\n#### tensorboard_cost_function\n![tensorboard_cost_function](https://i.imgur.com/yTCklib.png)\n#### tensorboard_biases_distribution\n![tensorboard_biases_distribution](https://i.imgur.com/iyZupnI.png)\n#### tensorboard_weights_distribution\n![tensorboard_weights_distribution](https://i.imgur.com/DxgMGZt.png)\n#### tensorboard_biases_histogram\n![tensorboard_biases_histogram](https://i.imgur.com/Af06kgc.png)\n#### tensorboard_weights_histogram\n![tensorboard_weights_histogram](https://i.imgur.com/wcbcIdy.png)"},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"959dd1f641e9dad526cd8e5299603db20d2a59ec"},"cell_type":"code","source":"# optionally run the command in the notebook itself by uncommenting the line below\n#!tensorboard --logdir=./logs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e181622ff4f8222f80cb836d700b2b0c941abcbe"},"cell_type":"markdown","source":"# Future Learning\n* What is PyTorch and how does it compare to Tensorflow?\n   * https://www.youtube.com/watch?v=nbJ-2G2GXL0\n     * Would PyTorch reduce the need to define \"placeholders\" because that was, frankly, weird to see in a language like python?\n* [But what *is* a Neural Network?](https://youtu.be/aircAruvnKk)\n  * [and how do they learn?](https://youtu.be/IHZwWFHWa-w) \n  * [and what is backprop really doing?](https://youtu.be/Ilg3gGewQ5U)\n  * [and how does backprop use calculus?](https://youtu.be/tIeHLnjs5U8)\n* How can the accuracy found above ```0.9254``` be improved to closer to ```0.95``` or ```0.99```?\n   * To what extend does changing the ```learning_rate``` or ```training_iteration``` or ```batch_size``` affect the accuracy? \n     * I dont think batch_size should have any affect. \n     * with ```training_iteration=30``` and ```learning_rate=0.01``` the algorithm ran in less than a few minutes and achieved ```0.9254```. Perhaps allowing it to train for several hours would boost the accuracy?\n       * [relevant quora question](http://qr.ae/TUGJid)\n* How long would it take a human toddler to \"classify\" digits (0-9)? An hour or two? maybe less? Of course you would need to hold their attention to the task, haha! \n* What would happen to the accuracy if I modified the test data or the training data to include **random noise** or even attempt the [**one pixel attack**](https://arxiv.org/abs/1710.08864)\n  * [video about one pixel attack](https://youtu.be/SA4YEAWVpbk)\n>   \"Now, note that this also means that we have to be able to look into the neural network and have access to the confidence values.\" - [KÃ¡roly Zsolnai-FehÃ©r](https://youtu.be/SA4YEAWVpbk?t=155)\n  *  Would a method for reducing NerualNet accuracy that only sees output classes, *without accuracy values*, be analogous to humans discovering optical illusions? haha! ðŸ˜‚\n  * [it seems some researchers have tried to trick AI that learned on MNIST data](https://arxiv.org/pdf/1801.02612.pdf)\n  * [another one](https://arxiv.org/pdf/1608.04644.pdf)\n  * [and another one](https://arxiv.org/pdf/1807.10335.pdf)\n  * [and more](https://www.google.com/search?safe=off&q=one+pixel+attack+\"mnist\")\n* What would happen to the accuracy if I changed **```tf.nn.softmax```** to **```relu```** or even **```sigmoid```** or **```tanh```**?\n  * [learn more about activation functions](https://youtu.be/-7scQpJT7uo)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}